{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%env CUDA_VISIBLE_DEVICES=1"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [],
            "source": [
                "import dataclasses\n",
                "\n",
                "import jax\n",
                "\n",
                "from openpi.models import model as _model\n",
                "from openpi.policies import droid_policy\n",
                "from openpi.policies import policy_config as _policy_config\n",
                "from openpi.shared import download\n",
                "from openpi.training import config as _config\n",
                "from openpi.training import data_loader as _data_loader"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Policy inference\n",
                "\n",
                "The following example shows how to create a policy from a checkpoint and run inference on a dummy example."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some kwargs in processor config are unused and will not have any effect: scale, action_dim, time_horizon, vocab_size, min_token. \n",
                        "Some kwargs in processor config are unused and will not have any effect: scale, action_dim, time_horizon, vocab_size, min_token. \n"
                    ]
                },
                {
                    "ename": "XlaRuntimeError",
                    "evalue": "INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"NVIDIA GeForce RTX 4090\"; cannot run it on device CUDA:1 of type \"NVIDIA GeForce RTX 3090\"",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mXlaRuntimeError\u001b[39m                           Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\u001b[39;00m\n\u001b[32m      8\u001b[39m example = droid_policy.make_droid_example()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m result = \u001b[43mpolicy\u001b[49m\u001b[43m.\u001b[49m\u001b[43minfer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     11\u001b[39m \u001b[38;5;66;03m# Delete the policy to free up memory.\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m policy\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Research/libs/openpi/src/openpi/policies/policy.py:53\u001b[39m, in \u001b[36mPolicy.infer\u001b[39m\u001b[34m(self, obs)\u001b[39m\n\u001b[32m     49\u001b[39m start_time = time.monotonic()\n\u001b[32m     50\u001b[39m \u001b[38;5;28mself\u001b[39m._rng, sample_rng = jax.random.split(\u001b[38;5;28mself\u001b[39m._rng)\n\u001b[32m     51\u001b[39m outputs = {\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m: inputs[\u001b[33m\"\u001b[39m\u001b[33mstate\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mactions\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_actions\u001b[49m\u001b[43m(\u001b[49m\u001b[43msample_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mObservation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sample_kwargs\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[32m     54\u001b[39m }\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Unbatch and convert to np.ndarray.        # Unbatch and convert to np.ndarray.\u001b[39;00m\n\u001b[32m     56\u001b[39m outputs = jax.tree.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: np.asarray(x[\u001b[32m0\u001b[39m, ...]), outputs)\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Research/libs/openpi/src/openpi/shared/nnx_utils.py:41\u001b[39m, in \u001b[36mmodule_jit.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     39\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(meth)\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapper\u001b[39m(*args: P.args, **kwargs: P.kwargs) -> R:\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mjitted_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
                        "    \u001b[31m[... skipping hidden 5 frame]\u001b[39m\n",
                        "\u001b[36mFile \u001b[39m\u001b[32m~/Research/libs/openpi/.venv/lib/python3.11/site-packages/jax/_src/interpreters/pxla.py:1297\u001b[39m, in \u001b[36mExecuteReplicated.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1295\u001b[39m   \u001b[38;5;28mself\u001b[39m._handle_token_bufs(result_token_bufs, sharded_runtime_token)\n\u001b[32m   1296\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1297\u001b[39m   results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mxla_executable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute_sharded\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_bufs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dispatch.needs_check_special():\n\u001b[32m   1300\u001b[39m   out_arrays = results.disassemble_into_single_device_arrays()\n",
                        "\u001b[31mXlaRuntimeError\u001b[39m: INVALID_ARGUMENT: executable is built for device CUDA:0 of type \"NVIDIA GeForce RTX 4090\"; cannot run it on device CUDA:1 of type \"NVIDIA GeForce RTX 3090\""
                    ]
                }
            ],
            "source": [
                "config = _config.get_config(\"pi0_fast_droid\")\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_fast_droid\")\n",
                "\n",
                "# Create a trained policy.\n",
                "policy = _policy_config.create_trained_policy(config, checkpoint_dir)\n",
                "\n",
                "# Run inference on a dummy example. This example corresponds to observations produced by the DROID runtime.\n",
                "example = droid_policy.make_droid_example()\n",
                "result = policy.infer(example)\n",
                "\n",
                "# Delete the policy to free up memory.\n",
                "del policy\n",
                "\n",
                "print(\"Actions shape:\", result[\"actions\"].shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Working with a live model\n",
                "\n",
                "\n",
                "The following example shows how to create a live model from a checkpoint and compute training loss. First, we are going to demonstrate how to do it with fake data.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "config = _config.get_config(\"pi0_aloha_sim\")\n",
                "\n",
                "checkpoint_dir = download.maybe_download(\"gs://openpi-assets/checkpoints/pi0_aloha_sim\")\n",
                "key = jax.random.key(0)\n",
                "\n",
                "# Create a model from the checkpoint.\n",
                "model = config.model.load(_model.restore_params(checkpoint_dir / \"params\"))\n",
                "\n",
                "# We can create fake observations and actions to test the model.\n",
                "obs, act = config.model.fake_obs(), config.model.fake_act()\n",
                "\n",
                "# Sample actions from the model.\n",
                "loss = model.compute_loss(key, obs, act)\n",
                "print(\"Loss shape:\", loss.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Now, we are going to create a data loader and use a real batch of training data to compute the loss."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [
                {
                    "ename": "NameError",
                    "evalue": "name 'config' is not defined",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Reduce the batch size to reduce memory usage.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m config = dataclasses.replace(\u001b[43mconfig\u001b[49m, batch_size=\u001b[32m2\u001b[39m)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load a single batch of data. This is the same data that will be used during training.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# NOTE: In order to make this example self-contained, we are skipping the normalization step\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# since it requires the normalization statistics to be generated using `compute_norm_stats`.\u001b[39;00m\n\u001b[32m      7\u001b[39m loader = _data_loader.create_data_loader(config, num_batches=\u001b[32m1\u001b[39m, skip_norm_stats=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
                        "\u001b[31mNameError\u001b[39m: name 'config' is not defined"
                    ]
                }
            ],
            "source": [
                "# Reduce the batch size to reduce memory usage.\n",
                "config = dataclasses.replace(config, batch_size=2)\n",
                "\n",
                "# Load a single batch of data. This is the same data that will be used during training.\n",
                "# NOTE: In order to make this example self-contained, we are skipping the normalization step\n",
                "# since it requires the normalization statistics to be generated using `compute_norm_stats`.\n",
                "loader = _data_loader.create_data_loader(config, num_batches=1, skip_norm_stats=True)\n",
                "obs, act = next(iter(loader))\n",
                "\n",
                "# Sample actions from the model.\n",
                "loss = model.compute_loss(key, obs, act)\n",
                "\n",
                "# Delete the model to free up memory.\n",
                "del model\n",
                "\n",
                "print(\"Loss shape:\", loss.shape)"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "openpi",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.13"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
